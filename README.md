# KernelPilot-V1-Basic-CUDA-Samples


| Task ID | Task | Prompt | Speedup (GTX 1660 SUPER GPU) | Speedup (RTX 3090 Ti GPU) |
|:-:|:-:|:-|:-:|:-:|
1 | Sigmoid | Implement a CUDA program for sigmoid activation function: $\text{sigmoid}(x) = 1 / (1 + \exp(-x))$. Input shape: (batch\_size, dim); Output: same shape as input. | 1.2001 | 1.0678 |
2 | Matrix Multiplication | Write a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix $A$ of dimensions $M \times K$ and matrix $B$ of dimensions $K \times N$, compute the product matrix $C = A \times B$, which will have dimensions $M \times N$. | 1.4224 | 5.2616 |
3 | Max Pooling 3D | Implement a CUDA program for 3D max pooling function that selects the maximum value within a defined region (a window) of a feature map. Input shape: (batch\_size, channels, dim1, dim2, dim3); Output: with 3D max pooling applied. | 1.1307 | 1.0233 | 
4 | LayerNorm | Implement a GPU program that performs Layer Normalization (LayerNorm) operation, which normalizes across the features for each individual data sample in a layer. Input of shape (batch\_size, features, dim1, dim2); Output with Layer Normalization applied, same shape as input. | 13.0792 | 22.3512 |
5 | 2D Convolution | Write a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a "valid" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: (1) input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. (2) kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output\_rows = input\_rows - kernel\_rows + 1, output\_cols = input\_cols - kernel\_cols + 1. The convolution operation is defined as: $output[i][j] = \sum_{m=0}^{kernel\_{rows}-1} \sum_{n=0}^{kernel\_{cols}-1} input[i+m][j+n] * kernel[m][n]$. | 1.0213 | 1.0457 |
6 | Multi-Head Self-Attention | Implement a CUDA program for multi-head self-attention. Given three input matrices $Q$ (queries), $K$ (keys), and $V$ (values) of size $N \times d_{\text{model}}$, compute: $\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1,\ldots,\text{head}_h)$, where each head computes: $\text{head}_{i} = \text{softmax}(\frac{Q_{i} K_{i}^{T}}{\sqrt{d_{k}}})V_{i}$ with $d_{k} = d_{\text{model}}/h$ and $Q_i$, $K_i$, $V_i$ being the $i$-th head's partition of the input matrices. | 0.6985 | 11.1264 |
7 | Mean Square Error | Implement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: $\text{MSE}=\frac{1}{N}\sum_{i=1}^{N}(predictions_{i}-targets_{i})^2$ where $N$ is the number of elements in each array. Input: predictions, targets; Output: MSE. | 27.8118 | 36.7118 |
8 | Matrix Transpose | Write a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix $A$ of dimensions rows $\times$ cols, the transpose $A^T$ will have dimensions cols $\times$ rows. All matrices are stored in row-major format.  | 1.7729 | 1.5634 |
9 | Reverse Array | Implement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input.  | 1.0310 | 1.1594 |
10 | ReLU Activation Fuction | Implement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: $\text{ReLU}(x)=\max(0,x)$. | 1.2440 | 1.0460 |
11 | Top-K Selection | Implement a GPU program that, given a 1D array input of 32-bit floating point numbers of length $N$, selects the $k$ largest elements and writes them in descending order to the output array of length $k$. | 6.0649 | 5.9551 |
12 | Sorting | Write a CUDA program that sorts an array of 32-bit floating-point numbers in ascending order using the bubble sort algorithm. Do not use other algorithms. | 1.3100 | 1.1405 |
13 | Matrix Copy | Implement a program that copies an $N \times N$ matrix of 32-bit floating point numbers from input array $A$ to output array $B$ on the GPU. The program should perform a direct element-wise copy so that $B_{i,j} = A_{i,j}$ for all valid indices. | 1.3686 | 1.0944 |
14 | Reduction | Write a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. | 40.0999 | 41.2879 |
15 | Dot Product | Implement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors $A$ and $B$ of length $n$ is defined as: $A \cdot B = \sum_{i=0}^{n-1} A_i \cdot B_i$. | 59.9929 | 102.3369 |
16 | Prefix Sum | Write a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array $[a, b, c, d, \ldots]$, the prefix sum is $[a, a+b, a+b+c, a+b+c+d, \ldots]$. | 0.9991 | 0.9020 |
17 | Categorical Cross-Entropy Loss | Implement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits $Z$ of size $N \times C$ and a vector of true class labels true\_labels of size $N$, compute the average cross-entropy loss over the batch. The loss for a single sample $j$ with logits $z_j = [z_{j1}, \ldots, z_{jC}]$ and true label $y_j$ is calculated using the numerically stable formula: $\text{Loss}_{j} = \log(\sum_{k=1}^{C} e^{z_{jk}}) - z_{j, y_j}$. The final output stored in the loss variable should be the average loss over the $N$ samples: $L = \frac{1}{N} \sum_{j=1}^{N} \text{Loss}_j$. Input: logits, true\_labels, $N$ (number of samples), and $C$ (number of classes). Output: loss (a pointer to a single float). | 12.9510 | 1.9592 |
18 | Monte Carlo Integration | Implement Monte Carlo integration on a GPU. Given a set of function values $y_i=f(x_i)$ sampled at random points uniformly distributed in the interval $[a, b]$, estimate the definite integral: $\int_{a}^{b}f(x)dx\approx (b-a)\cdot\frac{1}{n}\sum_{i=1}^{N}y_i$. The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. | 100.8339 | 179.3016 |
19 | Histogramming | Write a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num\_bins). You are given an input array input of length $N$ and the number of bins num\_bins. The result should be an array of integers of length num\_bins, where each element represents the count of occurrences of its corresponding index in the input array. | 17.9868 | 25.4716 |
20 | Ordinary Least Squares Regression | Solve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix $X$ of size $n\_{samples} \times n\_{features}$ and a target vector $y$ of size $n\_{samples}$, compute the coefficient vector $\beta$ that minimizes the sum of squared residuals: $\min_{\beta} \|X\beta - y\|^2$. The closed-form solution to OLS is: $\beta = (X^TX)^{-1}X^Ty$. | 4.8564 | 68.3329 |
